{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().endswith('nb'):\n",
    "    os.chdir('..')\n",
    "\n",
    "\n",
    "from lib.pdf_converter import pdf_to_txt_list, pdf_to_img_list\n",
    "\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "NAME = \"cs685\"\n",
    "\n",
    "save_dir = os.path.join(DATA_DIR, NAME)\n",
    "url_list_path = os.path.join(save_dir, \"urls.txt\")\n",
    "\n",
    "def url_to_file_path(url):\n",
    "    return os.path.join(save_dir, \"pdfs\", url.split('/')[-1])\n",
    "\n",
    "def download_url(url):\n",
    "    # make save directory\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    file_path = url_to_file_path(url)\n",
    "    # check if file exists before downloading\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'wb') as f:\n",
    "            print(\"Downloading {}\".format(url))\n",
    "            f.write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PDF files\n",
    "urls = []\n",
    "with open(url_list_path, 'r') as f:\n",
    "    for line in f:\n",
    "        urls.append(line.strip())\n",
    "\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    download_url(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 514 PDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of PDF texts with keys as the PDF file names and page numbers with text as the values\n",
    "# text_dict = {\n",
    "    # page_name: {\n",
    "    #     \"link\": \"pdf_url#page=page_number\",\n",
    "    #     \"text\": \"text_on_page\"\n",
    "    #     \"embedding\": [x1, x2, x3, ...]\n",
    "    # }\n",
    "# }\n",
    "text_dict = {}\n",
    "\n",
    "for url in urls:\n",
    "    file_path = url_to_file_path(url)\n",
    "    if not os.path.exists(file_path):\n",
    "        download_url(url)\n",
    "    print(\"Processing {}\".format(file_path))\n",
    "    pages = pdf_to_txt_list(file_path)\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    for slide_num, page in enumerate(pages):\n",
    "        slide_num += 1\n",
    "        # Use regex to search for \"{digits}\\n\\n\\f\"\n",
    "        search_result = re.search(r'\\n\\n\\d+\\n\\n', page)\n",
    "        if search_result:\n",
    "            # Get the page number\n",
    "            page_num = int(search_result.group(0)[2:-2])\n",
    "            # Remove the page number from the text\n",
    "            page = page[:search_result.start()]\n",
    "        else:\n",
    "            print(\"Warning: No page number found for {} slide {}\".format(file_name, slide_num))\n",
    "        \n",
    "        page_name = f\"{file_name[:-4]}-{page_num}\"\n",
    "        link = f\"{url}#page={slide_num}\"\n",
    "        \n",
    "        text_dict[page_name] = {\n",
    "            \"link\": link,\n",
    "            \"text\": page,\n",
    "            \"file_path\": file_path,\n",
    "            \"slide_num\": slide_num\n",
    "        }\n",
    "    \n",
    "    slide_nums = []\n",
    "    keys = []\n",
    "    for key in text_dict:\n",
    "        if text_dict[key][\"file_path\"] == file_path:\n",
    "            slide_nums.append(text_dict[key][\"slide_num\"])\n",
    "            keys.append(key)\n",
    "    \n",
    "    img_files = pdf_to_img_list(file_path, slide_nums, os.path.join(save_dir, \"imgs\"))\n",
    "\n",
    "    assert len(img_files) == len(slide_nums), \"Number of images and slide numbers do not match\"\n",
    "\n",
    "    for key, img_file in zip(keys, img_files):\n",
    "        text_dict[key][\"img\"] = img_file\n",
    "\n",
    "# Save the dictionary to a json file with file name as the directory name\n",
    "with open(f\"data/{NAME}/texts.json\", 'w') as f:\n",
    "    json.dump(text_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 685 PDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/cs685/pdfs/00-intro.pdf\n",
      "Processing data/cs685/pdfs/01-lm.pdf\n",
      "Processing data/cs685/pdfs/02-neural-lms.pdf\n",
      "Processing data/cs685/pdfs/04-attention.pdf\n",
      "Processing data/cs685/pdfs/05-transformers.pdf\n",
      "Processing data/cs685/pdfs/decoding.pdf\n",
      "Processing data/cs685/pdfs/tokenization.pdf\n",
      "Processing data/cs685/pdfs/prompt_learning.pdf\n",
      "Processing data/cs685/pdfs/gen_eval.pdf\n",
      "Processing data/cs685/pdfs/multilingual.pdf\n",
      "Processing data/cs685/pdfs/retrieval-lms.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of PDF texts with keys as the PDF file names and page numbers with text as the values\n",
    "# text_dict = {\n",
    "    # page_name: {\n",
    "    #     \"link\": \"pdf_url#page=page_number\",\n",
    "    #     \"text\": \"text_on_page\"\n",
    "    #     \"embedding\": [x1, x2, x3, ...]\n",
    "    # }\n",
    "# }\n",
    "text_dict = {}\n",
    "\n",
    "for url in urls:\n",
    "    file_path = url_to_file_path(url)\n",
    "    if not os.path.exists(file_path):\n",
    "        download_url(url)\n",
    "    print(\"Processing {}\".format(file_path))\n",
    "    pages = pdf_to_txt_list(file_path)\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    for slide_num, page in enumerate(pages):\n",
    "        if len(page) > 1000:\n",
    "            continue\n",
    "        slide_num += 1\n",
    "        page_name = f\"{file_name[:-4]}-{slide_num}\"\n",
    "        link = f\"{url}#page={slide_num}\"\n",
    "        \n",
    "        text_dict[page_name] = {\n",
    "            \"link\": link,\n",
    "            \"text\": page,\n",
    "            \"file_path\": file_path,\n",
    "            \"slide_num\": slide_num\n",
    "        }\n",
    "    \n",
    "    slide_nums = []\n",
    "    keys = []\n",
    "    for key in text_dict:\n",
    "        if text_dict[key][\"file_path\"] == file_path:\n",
    "            slide_nums.append(text_dict[key][\"slide_num\"])\n",
    "            keys.append(key)\n",
    "    \n",
    "    img_files = pdf_to_img_list(file_path, slide_nums, os.path.join(save_dir, \"imgs\"))\n",
    "\n",
    "    assert len(img_files) == len(slide_nums), \"Number of images and slide numbers do not match\"\n",
    "\n",
    "    for key, img_file in zip(keys, img_files):\n",
    "        text_dict[key][\"img\"] = img_file\n",
    "\n",
    "# Save the dictionary to a json file with file name as the directory name\n",
    "with open(f\"data/{NAME}/texts.json\", 'w') as f:\n",
    "    json.dump(text_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7695cc187696ff247200b960f75fa081789a85d99b43380ee59841795bcea6a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
