{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if os.getcwd().endswith('nb'):\n",
    "    os.chdir('..')\n",
    "\n",
    "\n",
    "from lib.pdf_converter import pdf_to_txt_list\n",
    "\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "\n",
    "DATA_DIR = \"data\"\n",
    "NAME = \"cs514\"\n",
    "\n",
    "save_dir = os.path.join(DATA_DIR, NAME)\n",
    "url_list_path = os.path.join(save_dir, \"urls.txt\")\n",
    "\n",
    "def url_to_file_path(url):\n",
    "    return os.path.join(save_dir, \"pdfs\", url.split('/')[-1])\n",
    "\n",
    "def download_url(url):\n",
    "    # make save directory\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    "    file_path = url_to_file_path(url)\n",
    "    # check if file exists before downloading\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'wb') as f:\n",
    "            print(\"Downloading {}\".format(url))\n",
    "            f.write(requests.get(url).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the PDF files\n",
    "urls = []\n",
    "with open(url_list_path, 'r') as f:\n",
    "    for line in f:\n",
    "        urls.append(line.strip())\n",
    "\n",
    "\n",
    "\n",
    "for url in urls:\n",
    "    download_url(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data/cs514/pdfs/lecture1.pdf\n",
      "Processing data/cs514/pdfs/lecture2.pdf\n",
      "Processing data/cs514/pdfs/lecture3.pdf\n",
      "Processing data/cs514/pdfs/lecture4.pdf\n",
      "Processing data/cs514/pdfs/lecture5.pdf\n",
      "Warning: No page number found for lecture5.pdf slide 69\n",
      "Processing data/cs514/pdfs/lecture6.pdf\n",
      "Warning: No page number found for lecture6.pdf slide 30\n",
      "Processing data/cs514/pdfs/lecture7.pdf\n",
      "Processing data/cs514/pdfs/lecture8.pdf\n",
      "Processing data/cs514/pdfs/lecture9.pdf\n",
      "Processing data/cs514/pdfs/lecture10.pdf\n",
      "Processing data/cs514/pdfs/lecture11.pdf\n",
      "Processing data/cs514/pdfs/lecture12.pdf\n",
      "Processing data/cs514/pdfs/lecture13.pdf\n",
      "Warning: No page number found for lecture13.pdf slide 21\n",
      "Processing data/cs514/pdfs/lecture14.pdf\n",
      "Processing data/cs514/pdfs/lecture15.pdf\n",
      "Processing data/cs514/pdfs/lecture16.pdf\n",
      "Warning: No page number found for lecture16.pdf slide 50\n",
      "Processing data/cs514/pdfs/lecture17.pdf\n",
      "Processing data/cs514/pdfs/lecture18.pdf\n",
      "Processing data/cs514/pdfs/lecture19.pdf\n",
      "Processing data/cs514/pdfs/lecture20.pdf\n",
      "Processing data/cs514/pdfs/lecture21.pdf\n",
      "Processing data/cs514/pdfs/lecture22.pdf\n",
      "Processing data/cs514/pdfs/lecture23.pdf\n",
      "Processing data/cs514/pdfs/lecture24.pdf\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of PDF texts with keys as the PDF file names and page numbers with text as the values\n",
    "# text_dict = {\n",
    "    # page_name: {\n",
    "    #     \"link\": \"pdf_url#page=page_number\",\n",
    "    #     \"text\": \"text_on_page\"\n",
    "    #     \"embedding\": [x1, x2, x3, ...]\n",
    "    # }\n",
    "# }\n",
    "text_dict = {}\n",
    "\n",
    "for url in urls:\n",
    "    file_path = url_to_file_path(url)\n",
    "    if not os.path.exists(file_path):\n",
    "        download_url(url)\n",
    "    print(\"Processing {}\".format(file_path))\n",
    "    pages = pdf_to_txt_list(file_path)\n",
    "    file_name = file_path.split('/')[-1]\n",
    "    for slide_num, page in enumerate(pages):\n",
    "        slide_num += 1\n",
    "        # Use regex to search for \"{digits}\\n\\n\\f\"\n",
    "        search_result = re.search(r'\\n\\n\\d+\\n\\n', page)\n",
    "        if search_result:\n",
    "            # Get the page number\n",
    "            page_num = int(search_result.group(0)[2:-2])\n",
    "            # Remove the page number from the text\n",
    "            page = page[:search_result.start()]\n",
    "        else:\n",
    "            print(\"Warning: No page number found for {} slide {}\".format(file_name, slide_num))\n",
    "        \n",
    "        page_name = f\"{file_name[:-4]}-{page_num}\"\n",
    "        link = f\"{url}#page={slide_num}\"\n",
    "        \n",
    "        text_dict[page_name] = {\n",
    "            \"link\": link,\n",
    "            \"text\": page\n",
    "        }\n",
    "\n",
    "# Save the dictionary to a json file with file name as the directory name\n",
    "with open(f\"data/{NAME}/texts.json\", 'w') as f:\n",
    "    json.dump(text_dict, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7695cc187696ff247200b960f75fa081789a85d99b43380ee59841795bcea6a6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
